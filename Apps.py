import cv2
import numpy as np
import pandas as pd
import streamlit as st
from PIL import Image
import os
import easyocr

# âœ… I'm mapping YOLO class IDs to meaningful labels
class_map = {
    0: "Test Name",
    1: "Value",
    2: "Units",
    3: "Reference Range"
}

# âœ… I'm loading YOLOv5 model in ONNX format
def load_yolo_model():
    model_path = "best.onnx"
    if not os.path.exists(model_path):
        st.error("Model file 'best.onnx' not found.")
        st.stop()
    model = cv2.dnn.readNetFromONNX(model_path)
    model.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
    model.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
    return model

# âœ… I'm performing YOLO inference after preprocessing
def predict_yolo(model, image):
    h, w = image.shape[:2]
    max_rc = max(h, w)
    input_img = np.zeros((max_rc, max_rc, 3), dtype=np.uint8)
    input_img[0:h, 0:w] = image
    blob = cv2.dnn.blobFromImage(input_img, 1/255, (640, 640), swapRB=True, crop=False)
    model.setInput(blob)
    preds = model.forward()
    return preds, input_img

# âœ… I'm processing predictions using confidence thresholds
def process_predictions(preds, input_img, conf_thresh=0.4, score_thresh=0.25):
    boxes, confidences, class_ids = [], [], []
    detections = preds[0]
    h, w = input_img.shape[:2]
    x_factor = w / 640
    y_factor = h / 640
    for det in detections:
        conf = det[4]
        if conf > conf_thresh:
            scores = det[5:]
            class_id = np.argmax(scores)
            if scores[class_id] > score_thresh:
                cx, cy, bw, bh = det[:4]
                x = int((cx - bw / 2) * x_factor)
                y = int((cy - bh / 2) * y_factor)
                boxes.append([x, y, int(bw * x_factor), int(bh * y_factor)])
                confidences.append(float(conf))
                class_ids.append(class_id)
    indices = cv2.dnn.NMSBoxes(boxes, confidences, score_thresh, 0.45)
    return indices.flatten() if len(indices) > 0 else [], boxes, class_ids

# âœ… I'm extracting text using EasyOCR or Pytesseract
def extract_fields_exploded(image, boxes, indices, class_ids, reader):
    results = {k: [] for k in class_map.values()}
    for i in indices:
        if i >= len(boxes): continue
        x, y, w, h = boxes[i]
        label = class_map.get(class_ids[i])
        if not label: continue
        crop = image[y:y+h, x:x+w]
        gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)
        gray = cv2.resize(gray, None, fx=2.5, fy=2.5, interpolation=cv2.INTER_CUBIC)
        blur = cv2.GaussianBlur(gray, (5, 5), 0)
        _, binary = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        roi = cv2.bitwise_not(binary)
        try:
            ocr_lines = reader.readtext(roi, detail=0)
        except:
            ocr_lines = []
        for line in ocr_lines:
            clean = line.strip()
            if clean:
                results[label].append(clean)
    return pd.DataFrame({col: pd.Series(vals) for col, vals in results.items()})

# âœ… I'm merging fragmented Test Name fields
def merge_fragmented_test_names(df):
    rows = df.to_dict("records")
    merged_rows, buffer = [], None
    for row in rows:
        if row.get("Test Name") and not any([row.get("Value"), row.get("Units"), row.get("Reference Range")]):
            buffer = row if not buffer else {"Test Name": buffer["Test Name"] + " " + row["Test Name"]}
        else:
            if buffer:
                merged_rows.append(buffer)
                buffer = None
            merged_rows.append(row)
    if buffer:
        merged_rows.append(buffer)
    return pd.DataFrame(merged_rows)

# âœ… I'm drawing bounding boxes on image
def draw_boxes(image, boxes, indices):
    for i in indices:
        x, y, w, h = boxes[i]
        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)
    return image

# âœ… Streamlit UI setup
st.set_page_config(page_title="Lab Report OCR", layout="centered", page_icon="ğŸ§¾")
st.markdown("<h2 style='text-align: center;'>ğŸ§¾ Lab Report OCR Extractor</h2>", unsafe_allow_html=True)

# ğŸ“¥ Google Drive link to test lab reports
st.markdown(
    "<div style='text-align:center;'>ğŸ“¥ <b>Download sample Lab Reports (JPG)</b> to test and upload from this: "
    "<a href='https://drive.google.com/drive/folders/1zgCl1A3HIqOIzgkBrWUFRhVV0dJZsCXC?usp=sharing' target='_blank'>Drive Link</a></div><br>",
    unsafe_allow_html=True
)

# ğŸ§  OCR Engine toggle, perfectly center aligned
st.markdown("<div style='text-align:center;'>ğŸ§  <b>Select OCR Engine</b></div>", unsafe_allow_html=True)
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    ocr_engine = st.radio("", ["EasyOCR", "Pytesseract"], index=0, horizontal=True, label_visibility="collapsed")

if ocr_engine == "Pytesseract":
    st.markdown("<div style='text-align:center; color:gray;'>âš ï¸ Must be installed at: <code>C:\\Program Files\\Tesseract-OCR\\tesseract.exe</code></div>", unsafe_allow_html=True)

# ğŸ“˜ Info section
with st.expander("ğŸ“˜ How it works", expanded=False):
    st.markdown("""
    1. Upload `.jpg`, `.jpeg`, or `.png` lab reports.
    2. YOLOv5 detects fields like Test Name, Value, Units, Reference Range.
    3. OCR engine extracts the text from fields.
    4. Split test names are intelligently merged.
    5. Download CSV or view detection overlay image.
    """)

# ğŸ“¤ Upload instruction
st.markdown("<div style='text-align:center;'>ğŸ“¤ Upload .jpg, .jpeg or .png lab reports<br>ğŸ“‚ Please upload one or more image files to begin.</div>", unsafe_allow_html=True)

# ğŸ–¼ File uploader supports jpg, jpeg, png
uploaded_files = st.file_uploader(" ", type=["jpg", "jpeg", "png"], accept_multiple_files=True)

# âœ… Processing uploaded files
if uploaded_files:
    model = load_yolo_model()
    reader = easyocr.Reader(['en'], gpu=False)

    for file in uploaded_files:
        st.markdown(f"---\n### ğŸ“„ Processing File: `{file.name}`")
        image = np.array(Image.open(file).convert("RGB"))

        with st.spinner("ğŸ” Running YOLOv5 Detection and OCR..."):
            st.markdown("<div style='text-align:center;'>ğŸ” Running YOLOv5 Detection and OCR...</div>", unsafe_allow_html=True)
            preds, input_img = predict_yolo(model, image)
            indices, boxes, class_ids = process_predictions(preds, input_img)

            if len(indices) == 0:
                st.warning("âš ï¸ No fields detected in this image.")
                continue

            df = extract_fields_exploded(image, boxes, indices, class_ids, reader)
            df = merge_fragmented_test_names(df)

        st.success("âœ… Extraction Complete!")

        # ğŸ§¾ Show extracted table
        st.markdown("<h5 style='text-align:center;'>ğŸ§¾ Extracted Table</h5>", unsafe_allow_html=True)
        st.dataframe(df, use_container_width=True)

        # ğŸ“¦ Show detection overlay image
        st.markdown("<h5 style='text-align:center;'>ğŸ“¦ Detected Fields on Image</h5>", unsafe_allow_html=True)
        st.image(draw_boxes(image.copy(), boxes, indices), use_container_width=True)

        # ğŸ“¥ Download and Reset (centered)
        col4, col5, col6 = st.columns([1, 2, 1])
        with col5:
            st.download_button("â¬‡ï¸ Download CSV", df.to_csv(index=False), file_name=f"{file.name}_ocr.csv", mime="text/csv")
            if st.button("ğŸ”„ Reset All"):
                st.session_state.clear()
                st.experimental_rerun()
