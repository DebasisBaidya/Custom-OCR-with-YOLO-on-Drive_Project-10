# ğŸ©º Custom Object Character Recognition (OCR) on Google Drive

<p align="center">
  <img src="https://img.shields.io/badge/Python-Core_Programming-3776AB?logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/OpenCV-Image_Processing-5C3EE8?logo=opencv&logoColor=white" />
  <img src="https://img.shields.io/badge/NumPy-Numerical_Computation-4D77CF?logo=numpy&logoColor=white" />
  <img src="https://img.shields.io/badge/Pandas-Data_Handling-150458?logo=pandas&logoColor=white" />
  <img src="https://img.shields.io/badge/PIL-Image_Reading-FD9F00?logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/Tesseract_OCR-Text_Recognition-FFBB00?logo=tesseract&logoColor=black" />
  <img src="https://img.shields.io/badge/PyTesseract-OCR_API-F89820?logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/EasyOCR-Deep_OCR-FF6600?logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/YOLOv3-Object_Detection-28A745?logo=yolo&logoColor=white" />
  <img src="https://img.shields.io/badge/ONNX-Model_Deployment-9058B4?logo=onnx&logoColor=white" />
  <img src="https://img.shields.io/badge/Streamlit-Web_App_UI-FF4B4B?logo=streamlit&logoColor=white" />
</p>

#

Welcome to my **Medical Lab Report OCR Project**! ğŸ§ª

This project tackles a very specific problem in medical documentation: extracting structured data like **Test Names**, **Values**, **Units**, and **Reference Ranges** from unstructured lab report images.

Manually entering such data is slow, repetitive, and error-prone. So I designed an intelligent solution combining object detection (YOLOv3) with OCR (Tesseract/EasyOCR) to automate this workflow. ğŸ§ ğŸ–¼ï¸

This is not just a proof of concept â€” it's a fully working pipeline with model training, preprocessing, post-processing, UI deployment, and CSV export.

- > ğŸ”— Try the app: [ğŸŒ Streamlit Live](https://custom-ocr-with-yolo-on-drive-debasis-baidya.streamlit.app/)
- > ğŸ¬ Watch Demo: [ğŸ“½ YouTube Video](https://your-demo-video-link)


---

## ğŸ¯ Objective

Build an end-to-end system to:

* ğŸ¯ **Detect**: Identify specific regions in lab reports using YOLOv3
* ğŸ“¸ **Preprocess**: Clean and prepare those image snippets
* ğŸ§¾ **Extract**: Use Tesseract/EasyOCR to pull out actual text from regions
* ğŸ§  **Structure**: Organize results into a tabular format
* ğŸ“¤ **Export**: Save as downloadable CSV for use in Excel or EMR systems
* ğŸ’» **Deploy**: Provide a UI via Streamlit so others can test it easily

---

## ğŸ§  Problem Background

In real-world clinics and diagnostic labs, lab reports are often scanned or saved as JPGs with varied formats. Extracting key medical parameters is essential but mostly done manually.

This project solves that by:

* ğŸ” Detecting predefined regions using a trained YOLOv3 model
* ğŸ§¾ Reading those regions via OCR (Tesseract/EasyOCR)
* ğŸ“Š Structuring the extracted text for automated storage and analysis

---

## ğŸ“‚ Approach Overview

The pipeline follows these logical steps:

1. ğŸ“Œ **Image Labeling** â€“ using `LabelImg` for bounding boxes
2. ğŸ“ **Annotation Conversion** â€“ XML to YOLO txt via custom Python script
3. ğŸ§  **YOLOv3 Training** â€“ on Google Colab (GPU enabled)
4. ğŸ”„ **Model Export** â€“ save best weights to ONNX
5. ğŸ” **Detection + Preprocessing** â€“ load model, preprocess with OpenCV
6. ğŸ§¾ **Text Recognition** â€“ via EasyOCR or Pytesseract
7. ğŸ§  **Smart Postprocessing** â€“ e.g., merging multi-word test names
8. ğŸ“¤ **CSV + Annotated Image Output**
9. ğŸŒ **Deployment** â€“ Streamlit-based web interface
10. ğŸ¥ **Documentation + Demo** â€“ screenshots, videos for review

---

## ğŸ”§ Features Implemented

* âœ… YOLOv3-based detection (ONNX)
* âœ… Custom image preprocessing: resize, grayscale, blur, threshold
* âœ… OCR with fallback: EasyOCR or Pytesseract
* âœ… Smart merge logic for split fields
* âœ… Streamlit UI with image upload, download buttons
* âœ… Annotated image preview + CSV export

---

## ğŸ“ File Structure

```
ğŸ“‚ project_root
â”œâ”€â”€ ğŸ“’ P10.ipynb                # Notebook for model training & XML parsing
â”œâ”€â”€ ğŸ“„ Apps.py                 # Streamlit app for UI
â”œâ”€â”€ ğŸ§¾ data.yaml              # YOLO label map config
â”œâ”€â”€ ğŸ§  model/best.onnx        # Trained YOLO model (ONNX format)
â”œâ”€â”€ ğŸ“· data_images/           # Labeled input images
â”œâ”€â”€ ğŸ“„ Extract Text from XML.ipynb  # Converts XML â†’ YOLO format
â”œâ”€â”€ ğŸ“¸ screenshots/            # Add your screenshots here
â””â”€â”€ ğŸ“„ Instructions.pdf        # Project outline
```

---

## ğŸ“Š Results

* ğŸ“Œ **Accuracy**: High mAP\@0.5 for bounding box detection
* ğŸ§  **OCR Success Rate**: Robust across different font sizes/styles
* ğŸ”„ **Smart Field Merging**: e.g., joins "Total" + "Bilirubin"
* ğŸ“ˆ **Usability**: Streamlit UI makes it plug-and-play for any JPG report

---

## ğŸ–¼ï¸ Streamlit Interface (UI Preview)

| Input Image                     | YOLO Detection                     | OCR Output                        |
| ------------------------------- | ---------------------------------- | --------------------------------- |
| ![input](Screenshots/input.jpg) | ![yolo](Screenshots/detection.jpg) | ![output](Screenshots/output.jpg) |

> ğŸ“¦ Exports: `ocr_result.csv`, `annotated_image.jpg`

---

## ğŸ’¡ What I Did â€” Full Technical Walkthrough

### ğŸ§· 1. Image Labeling

```bash
ğŸ–¼ï¸ Labeled the report images using LabelImg  
ğŸ·ï¸ Saved bounding boxes for 4 classes: Test Name, Value, Units, Reference Range  
ğŸ’¾ Output saved in Pascal VOC (XML) format
```

---

### ğŸ“¤ 2. Convert Annotations (XML â†’ YOLO txt)

```python
ğŸ“‚ Used Extract_Text_from_XML.ipynb to parse XML files  
ğŸ§¾ Extracted filenames, dimensions, and bounding boxes  
ğŸ“ Normalized coordinates and encoded labels for YOLOv5 format  
ğŸ“ Saved them as .txt files (YOLO format)
```

---

### âš™ï¸ 3. Model Training on Google Colab (GPU)

```bash
ğŸ’¾ Mounted Google Drive
from google.colab import drive
drive.mount('/content/drive')

ğŸ§¬ Cloned YOLOv5 repo and installed dependencies
!git clone https://github.com/ultralytics/yolov5.git
%cd yolov5
!pip install -r requirements.txt

ğŸš€ Trained YOLOv5 model with custom dataset
!python train.py --data data.yaml --weights runs/train/Model2/weights/best.pt --img 640 --batch-size 2 --name Model --epochs 200
```

âœ… Training was done on **Google Colab (GPU runtime)** for speed and convenience.

---

### ğŸ§  4. Export Model to ONNX

```bash
ğŸ“¤ Exported best weights to ONNX format for OpenCV inference
# âœ… Set path to your trained weights
best_pt_path = "/content/drive/MyDrive/CustomOCR/yolov5/runs/train/Model5/weights/best.pt"

# âœ… Changed directory to yolov5 before running export
%cd /content/drive/MyDrive/CustomOCR/yolov5

# âœ… Run export to generate ONNX model in same folder as best.pt
!python export.py --weights {best_pt_path} --include onnx --simplify --opset 12

# âœ… Moved ONNX model to /models/
!mv runs/train/Model5/weights/best.onnx /content/drive/MyDrive/CustomOCR/models/best.onnx
```

---

### ğŸ” 5. Inference + Preprocessing (OpenCV DNN)

```python
ğŸ§  Used Apps.py and main.py for ONNX inference  
ğŸ“ Resized input image to 640x640  
ğŸ“ Applied padding to maintain aspect ratio  
ğŸ§± Created blob with OpenCV and ran model  
âœ‚ï¸ Applied NMS to filter overlapping detections
```

---

### ğŸ§¾ 6. OCR with Tesseract/EasyOCR

```python
ğŸ” Cropped detected boxes  
ğŸ–¤ Applied preprocessing:
    - Grayscale  
    - Gaussian Blur  
    - Threshold + Invert  

ğŸ§  Used EasyOCR (or PyTesseract) to extract text for each box
```

---

### ğŸ“Š 7. Post-Processing

```python
ğŸ”¤ Merged fragmented test names (e.g., "Total" + "Cholesterol")  
ğŸ§¾ Combined outputs into a structured pandas DataFrame  
ğŸ“¥ Provided download button for CSV
```

---

### ğŸŒ 8. Streamlit UI Deployment

```bash
ğŸ“¤ Upload JPG file  
ğŸ“¦ YOLOv5 ONNX model detects fields  
ğŸ”  OCR extracts the text  
ğŸ“Š Results shown in table format  
ğŸ–¼ï¸ Annotated image + ğŸ“„ CSV available to download  
â–¶ï¸ streamlit run Apps.py
```

---

## ğŸ–¼ï¸ Streamlit UI Screenshot

<p align="center">
  <img src="Screenshots/streamlit_full.png" alt="streamlit_ui" />
</p>

---

## ğŸ™‹â€â™‚ï¸ About Me

Hi, Iâ€™m **Debasis Baidya** from Kolkata ğŸ‘‹
With **11+ years** of experience in the MIS domain, I am now transitioning into the world of **Data Science**.

* Currently Working as Senior MIS Analyst | Data Science Intern

âœ… 80%+ automation of manual processes at my workplace
ğŸ“Š Skilled in Power BI, Python, SQL, ML, DL, NLP, Google Apps Script, Google Site

<p align="left">
  ğŸ“¢ <strong>Connect with me:</strong>&nbsp;

  <a href="https://www.linkedin.com/in/debasisbaidya">
    <img src="https://img.shields.io/badge/LinkedIn-View_Profile-blue?logo=linkedin&logoColor=white" />
  </a>

  <a href="mailto:speak2debasis@gmail.com">
    <img src="https://img.shields.io/badge/Gmail-Mail_Me-red?logo=gmail&logoColor=white" />
  </a>

  <a href="https://api.whatsapp.com/send?phone=918013316086&text=Hi%20Debasis!">
    <img src="https://img.shields.io/badge/WhatsApp-Message-green?logo=whatsapp&logoColor=white" />
  </a>
</p>


---

> ğŸš€ This project shows how machine learning, OCR, and a clean UI can turn static lab reports into structured, digital, and actionable data. Ideal for EMRs, diagnostic apps, and healthcare analytics. ğŸ¥

