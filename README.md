# ğŸ§¾ Custom Object Character Recognition (OCR) on Google Drive

<p align="center">
  <img src="https://img.shields.io/badge/OpenCV-Used-007ACC?logo=opencv&logoColor=white" />
  <img src="https://img.shields.io/badge/numpy-Used-4D77CF?logo=numpy&logoColor=white" />
  <img src="https://img.shields.io/badge/Pandas-Used-150458?logo=pandas&logoColor=white" />
  <img src="https://img.shields.io/badge/PyTesseract-Used-F89820?logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/EasyOCR-Used-FF6600?logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/TesseractOCR-Used-525252?logo=google&logoColor=white" />
  <img src="https://img.shields.io/badge/YOLOv3-Used-28A745?logo=yolo&logoColor=white" />
  <img src="https://img.shields.io/badge/ONNX-Used-9058B4?logo=onnx&logoColor=white" />
  <img src="https://img.shields.io/badge/Streamlit-Used-FF4B4B?logo=streamlit&logoColor=white" />
  <img src="https://img.shields.io/badge/PIL-Used-5C3EE8?logo=python&logoColor=white" />
</p>

Welcome to my **Medical Lab Report OCR Project**! ğŸ§ª

This project tackles a very specific problem in medical documentation: extracting structured data like **Test Names**, **Values**, **Units**, and **Reference Ranges** from unstructured lab report images.

Manually entering such data is slow, repetitive, and error-prone. So I designed an intelligent solution combining object detection (YOLOv3) with OCR (Tesseract/EasyOCR) to automate this workflow. ğŸ§ ğŸ–¼ï¸

This is not just a proof of concept â€” it's a fully working pipeline with model training, preprocessing, post-processing, UI deployment, and CSV export.

> ğŸ”— [ğŸŒ Streamlit App](https://your-streamlit-app-url) | ğŸ¥ [ğŸ“½ Demo Video](https://your-demo-video-link)

---

## ğŸ¯ Objective

Build an end-to-end system to:

* ğŸ¯ **Detect**: Identify specific regions in lab reports using YOLOv3
* ğŸ“¸ **Preprocess**: Clean and prepare those image snippets
* ğŸ§¾ **Extract**: Use Tesseract/EasyOCR to pull out actual text from regions
* ğŸ§  **Structure**: Organize results into a tabular format
* ğŸ“¤ **Export**: Save as downloadable CSV for use in Excel or EMR systems
* ğŸ’» **Deploy**: Provide a UI via Streamlit so others can test it easily

---

## ğŸ§  Problem Background

In real-world clinics and diagnostic labs, lab reports are often scanned or saved as JPGs with varied formats. Extracting key medical parameters is essential but mostly done manually.

This project solves that by:

* ğŸ” Detecting predefined regions using a trained YOLOv3 model
* ğŸ§¾ Reading those regions via OCR (Tesseract/EasyOCR)
* ğŸ“Š Structuring the extracted text for automated storage and analysis

---

## ğŸ“‚ Approach Overview

The pipeline follows these logical steps:

1. ğŸ“Œ **Image Labeling** â€“ using `LabelImg` for bounding boxes
2. ğŸ“ **Annotation Conversion** â€“ XML to YOLO txt via custom Python script
3. ğŸ§  **YOLOv3 Training** â€“ on Google Colab (GPU enabled)
4. ğŸ”„ **Model Export** â€“ save best weights to ONNX
5. ğŸ” **Detection + Preprocessing** â€“ load model, preprocess with OpenCV
6. ğŸ§¾ **Text Recognition** â€“ via EasyOCR or Pytesseract
7. ğŸ§  **Smart Postprocessing** â€“ e.g., merging multi-word test names
8. ğŸ“¤ **CSV + Annotated Image Output**
9. ğŸŒ **Deployment** â€“ Streamlit-based web interface
10. ğŸ¥ **Documentation + Demo** â€“ screenshots, videos for review

---

## ğŸ”§ Features Implemented

* âœ… YOLOv3-based detection (ONNX)
* âœ… Custom image preprocessing: resize, grayscale, blur, threshold
* âœ… OCR with fallback: EasyOCR or Pytesseract
* âœ… Smart merge logic for split fields
* âœ… Streamlit UI with image upload, download buttons
* âœ… Annotated image preview + CSV export

---

## ğŸ“ File Structure

```
ğŸ“‚ project_root
â”œâ”€â”€ ğŸ“’ P10.ipynb                # Notebook for model training & XML parsing
â”œâ”€â”€ ğŸ“„ Apps.py                 # Streamlit app for UI
â”œâ”€â”€ ğŸ§¾ data.yaml              # YOLO label map config
â”œâ”€â”€ ğŸ§  model/best.onnx        # Trained YOLO model (ONNX format)
â”œâ”€â”€ ğŸ“· data_images/           # Labeled input images
â”œâ”€â”€ ğŸ“„ Extract Text from XML.ipynb  # Converts XML â†’ YOLO format
â”œâ”€â”€ ğŸ“¸ screenshots/            # Add your screenshots here
â””â”€â”€ ğŸ“„ Instructions.pdf        # Project outline
```

---

## ğŸ–¼ï¸ Streamlit Interface (UI Preview)

| Upload & Annotate                               | Extracted Table & CSV                          | Preview Annotated Output                         |
| ----------------------------------------------- | ---------------------------------------------- | ------------------------------------------------ |
| ![streamlit1](screenshots/streamlit_upload.jpg) | ![streamlit2](screenshots/streamlit_table.jpg) | ![streamlit3](screenshots/streamlit_preview.jpg) |

> ğŸ”— Try the app: [ğŸŒ Streamlit Live](https://your-streamlit-app-url)
> ğŸ¬ Watch Demo: [ğŸ“½ YouTube Video](https://your-demo-video-link)

---

## ğŸ“Š Results

* ğŸ“Œ **Accuracy**: High mAP\@0.5 for bounding box detection
* ğŸ§  **OCR Success Rate**: Robust across different font sizes/styles
* ğŸ”„ **Smart Field Merging**: e.g., joins "Total" + "Bilirubin"
* ğŸ“ˆ **Usability**: Streamlit UI makes it plug-and-play for any JPG report

---

## ğŸ§ª Sample Output

| Input Image                     | YOLO Detection                     | OCR Output                        |
| ------------------------------- | ---------------------------------- | --------------------------------- |
| ![input](screenshots/input.jpg) | ![yolo](screenshots/detection.jpg) | ![output](screenshots/output.jpg) |

> ğŸ“¦ Exports: `ocr_result.csv`, `annotated_image.jpg`

---

## ğŸ™‹â€â™‚ï¸ About Me

Hi, Iâ€™m **Debasis Baidya** from Kolkata ğŸ‘‹
With **11+ years** of experience in the MIS domain, I am now transitioning into the world of **Data Science**.

* Currently Working as Senior MIS Analyst | Data Science Intern

âœ… 80%+ automation of manual processes at my workplace
ğŸ“Š Skilled in Power BI, Python, SQL, ML, DL, NLP, Google Apps Script, Google Site

<p align="left">
  ğŸ“¢ <strong>Connect with me:</strong>&nbsp;

  <a href="https://www.linkedin.com/in/debasisbaidya">
    <img src="https://img.shields.io/badge/LinkedIn-View_Profile-blue?logo=linkedin&logoColor=white" />
  </a>

  <a href="mailto:speak2debasis@gmail.com">
    <img src="https://img.shields.io/badge/Gmail-Mail_Me-red?logo=gmail&logoColor=white" />
  </a>

  <a href="https://api.whatsapp.com/send?phone=918013316086&text=Hi%20Debasis!">
    <img src="https://img.shields.io/badge/WhatsApp-Message-green?logo=whatsapp&logoColor=white" />
  </a>
</p>

---

## âš™ï¸ Try It Locally

```bash
# Run locally with image input
python main.py --image path/to/image.jpg

# Streamlit App
streamlit run Apps.py
```

---

## ğŸ’° Cost & Resources

* ğŸ’» Google Colab (Free Tier)
* ğŸš€ Training Time: \~2 hours for 200 epochs
* ğŸ’¸ Total Cost: â‚¹0 (under free limits)

---

> ğŸš€ This project shows how machine learning, OCR, and a clean UI can turn static lab reports into structured, digital, and actionable data. Ideal for EMRs, diagnostic apps, and healthcare analytics. ğŸ¥
